<!DOCTYPE html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
<style>
  #one:hover{
    color: red;
    text-decoration: none;
  }
  #two:hover{
    color: red;
    text-decoration: none;
  }
  #two{
    color: white;
  }
  #one{
    color: white;
  }
* {
  box-sizing: border-box;
 
}
.row{
  padding: 20px;
  padding-left: 40px;
}

/* Add a gray background color with some padding */
body {
  font-family: Arial;
  /* padding: 20px; */
  background: #f1f1f1;
}

/* Header/Blog Title */
.header {
  padding: 50px;
  font-size: 40px;
  text-align: center;
  background: white;
}

/* Create two unequal columns that floats next to each other */
/* Left column */
.leftcolumn {   
  float: left;
  width: 75%;
}

/* Right column */
.rightcolumn {
  float: left;
  width: 25%;
  padding-left: 20px;
}
.rightx{
display:flex;
justify-content: center;
align-items: center;
position: absolute;
top: 36%;
left: 80vw;
}

/* Add a card effect for articles */
.card {
   background-color: white;
   padding: 20px;
   margin-top: 20px;
   margin-bottom: 20px;
   display: flex;
   align-items: left;
   margin: 20px;
}

/* Clear floats after the columns */
.row:after {
  content: "";
  display: table;
  clear: both;
}

/* Footer */
.footer {
  padding: 20px;
  text-align: center;
  background: #ddd;
  margin-top: 20px;
}
table{
  border: 1px solid black;
  
  
}
th,td{
  border: 1px solid black;
}
.two{

  margin-top: 2%;
  margin-bottom: 2%;
}
/* Responsive layout - when the screen is less than 800px wide, make the two columns stack on top of each other instead of next to each other */
@media screen and (max-width: 800px) {
  .leftcolumn, .rightcolumn {   
    width: 100%;
    padding: 0;
  }
}
</style>
</head>
<body>
  <nav class="navbar navbar-light bg-dark  justify-content-between">
    <a class="navbar-brand " id="one" href="./../index.html">Home</a>
    
    <div class="form-inline">
      <a class="navbar-brand " id="two" href="./../allBlogLink.html">All Blogs</a>
    </div>
  </nav>
<div class="header mt-2">
  <h2>Convolution on Graphs(spectral Domains)</h2>
 
</div>


<div class="row">
  <div class="leftcolumn">
    <div class="card">
        <h2>Topic covered:</h2>
        <li>Introduction</li>
        <li>Spectral Domains</li>
        <li>Convolutional Graph Neural Networks</li>
        <li>Spectral CNN</li>
        <li>ChebNets or Chebyshev Spectral CNN</li>
        <li>CayleyNets</li>
        <Li>GCN</Li>
        <li>Special Points on GCN</li>
        <li>Some results and analysis</li>
        <li>Problem with spectral Domains</li>
        <li>Conclusions</li>
        <div><img src='Standardization-of-a-two-component-bivariate-Gaussian-mixture-model-with-outliers-a.png' height=300px /></div>
        <div>
            <h2>Note</h2>
            <p>This post includes only basic graph convolution algorithms. It donot include applications of graphs and other algorithms. </p>
        </div>
        <h2>Introduction</h2>
        <p> The  great success of Neural Networks for pattern recognition and data mininghas made the researchers to use it in areas like feature engineering, classification etc. 
            Deep architectures like CNN and RNN has revolutionized their use in every domainin which they can be used like text, images, video, signal etc. 
            CNN brought great success on image analysis. The grid structure of image make CNN to exploit local connectivity, translational
             invariance and shifting to produce meaningful embeddings(low dimensional features) of that image. 
             The growth in computational power and availability of big data are  also behind success story of deeplearning architectures.
        </p>
        <p>
            Researchers thought of using  convolution(CNN) on non-grid, irregular structures like graphs. As most ML algorithms needs euclidean distance as similarity notion.
            Same notion can't be utilised in graphs as it has relative positions.
            Motivated by CNN, an image can also be thought as special case of graphs where pixel are connected to adjacent pixels.
        </p>
        <p>
            In graph convolution, one may think as weighted average of node's neighbourhood information. In this 
            post, we will go through graph convolution which has origin in spectral domains.
        </p>
        <h2> Spectral Domains</h2>
        <p>Spectral based methods have solid backing of mathematical foundation in graph signal processing.
             It assumes that graph is undirected. It uses graph signals and eigen vectors  of graph Laplacian matrix. 
             The Global structure can be exploit with spectrum of graph Laplacian to generalize convolution operator.
             </p>
        <p>Mathematically Laplacian has these 3 different forms:
             <ul>
                 <li>
                     1
                 </li>
                 <li>
                     2
                 </li>
                 <li>
                     3
                 </li>
             </ul>
        </p>
        <p>
            Ideal behind using spectral domains is Convolution is easy in signal domain. One application where it is used predominantly is spectral clustering. 
            We will look into it in detail in some other post. 
            One can go through this paper by <a href ="https://arxiv.org/abs/0711.0189">Luxburg</a> for more details.  
        </p>

        <h2>Convolutional Graph Neural Networks</h2>
        <p>
            As in above paragraph we have different mathematical definitions for graph 
            Laplacian matrix. For simplicity take <br>
            L = <br>
            this is symmetrical Laplacian matrix, also a positive semidefinite matrix.<br>
            By diagonalization L = UDU^T
            <T></T>
        </p>
        <h2> When scaling is recommended?</h2>
        <p>
            <li>
                K-nearest neighbours and K-means is euclidean distance based algorithm. It is sensitive to magnitude of feature, hence it is always recommended to use scaling  before training. 
            </li>
            <li>
                Parameters/Weight based models that are optimised by gradient based approaches like Support Vector Machine(SVM), 
                Logistic regression, Neural NEtwork etc require scaling so that 
                weights are not updated faster with respect to feature with higher order of magnitude.
            </li>
            <li>
                Models like PCA, LDA  are also sensitive to feature magnitude order and depend on variance. PCA is one where scaling is strongly recommended. 
                In PCA we are interested in the components that maximize the variance. 
                PCA might determine that the direction of maximal variance more closely corresponds with the feature that have high variance, if this high variance is due to unscaling then PCA gives skews directions. We will get more clarity from <a ref="">here</a> about how scaling help.
            </li>
        </p>
        <h2>
            Standardization
        </h2>
        <p>
            Standardization is simple feature scaling technique where values are replaced by z-score.<br>
            X_new(Z-score) = (X-X_mean)/(sqrt(var(X)))<br>

            where X_mean is sampling mean and sqrt(var(x)) is sampling standard deviation.
            <br><br>
            By doing this, we make new value to be around zero centric mean and unit variance.<br>
            X_mean_new = 0<br>
            var(X_new) = 1 <br>
        </p>
        <h2>
            Min-max Scaling
        </h2>
        <P>
            Alternative to  z-score normalisation, Data is scaled in range from 0 to 1 usually.<br>
            X_new = (X - X_min)/(X_max - X_min)<br>
            Here we endup with smaller standard deviation and supress the effect of outliers.

        </P>
        <h2>Demonstration of scaling on PCA using Sklearn libraries.</h2>
        <P>
            The dataset used is the Wine Dataset available at UCI. This dataset has continuous features that are heterogeneous in scale due to differing properties that they measure (i.e alcohol content, and malic acid). 
            To test whether effect of scaling, we  can use classifier over PCA and analyse its performance. We have taken accuracy to analyse the performance. The classifier used is k-nearest neighbours.
            <div>
                <strong>Performance of scaled and unscaled PCA(dim=2) data using k-nearest neighbours as classifier</strong></div>
                <div class="two">
            <table>
                <tr>
                    <th>data </th>
                    <th>Accuracy</th>
                </tr>
                <tr>
                    <td>Unscaled data</td>
                    <td>61.11%</td>
                </tr>
                <tr>
                    <td>Scaled data</td>
                    <td>96.30%</td>
                </tr>
            </table>
          </div>
            We can see from table that scaled outperforms unscaled data.<br>
            We can also  note the difference from Plot defined below.
            <img src = "download (3).png"/>
            <div><strong>2D plot using first and second Principal Components for both unscaled and scaled data</strong></div>
        </P>
        <h2>
            Conclusions
        </h2>
        <p>
            <li>
                From demonstration, it is clear that feature scaling is useful.
            </li>
            <li>
                When in doubt, it is always recommended to use standardization. It will not hurt the performance atleast.
            </li>
            <li>
                If there are outliers in sample, it will make the sampling mean(mu_s) high which is incorrect and give false i. So it is always recommended to remove the outlier before applying min-max or standardization. 
                For removing outliers there are lot of information present on internet so one who is willing to explore can always find some good blogs.
            </li>
         </p>
    </div>

    <div>
        <h>Reference</h>
        <br>
        <a href="https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html">Importance of Feature Scaling(scikit-learn)</a>
        <br>
    </div>

  
    
    <div class="card">
      <h2>
        From
      </h2>
      <p>Swyam Prakash Singh</p>
      <p>ML enthusiast</p>
    </div>
 
</div>

</body>
</html>
